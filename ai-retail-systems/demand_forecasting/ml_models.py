"""
æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹éœ€è¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
Random Forestã€XGBoostã€LSTMãªã©ã®æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã‚’å®Ÿè£…
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import joblib
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class MLDemandForecaster:
    def __init__(self):
        """æ©Ÿæ¢°å­¦ç¿’éœ€è¦äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–"""
        self.models = {}
        self.scalers = {}
        self.feature_columns = []
        self.target_column = 'quantity'
        
    def load_and_prepare_data(self, file_path: str):
        """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        try:
            df = pd.read_csv(file_path)
            df['date'] = pd.to_datetime(df['date'])
            
            print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ãƒ¬ã‚³ãƒ¼ãƒ‰")
            return df
            
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def create_features(self, df):
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        print("ç‰¹å¾´é‡ã‚’ä½œæˆä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚³ãƒ”ãƒ¼
        feature_df = df.copy()
        
        # åŸºæœ¬çš„ãªæ™‚é–“ç‰¹å¾´é‡
        feature_df['year'] = feature_df['date'].dt.year
        feature_df['month'] = feature_df['date'].dt.month
        feature_df['day'] = feature_df['date'].dt.day
        feature_df['dayofweek'] = feature_df['date'].dt.dayofweek
        feature_df['dayofyear'] = feature_df['date'].dt.dayofyear
        feature_df['quarter'] = feature_df['date'].dt.quarter
        
        # é€±æœ«ãƒ•ãƒ©ã‚°
        feature_df['is_weekend'] = (feature_df['dayofweek'] >= 5).astype(int)
        
        # æœˆã®å§‹ã¾ã‚Šãƒ»çµ‚ã‚ã‚Šãƒ•ãƒ©ã‚°
        feature_df['is_month_start'] = (feature_df['day'] <= 5).astype(int)
        feature_df['is_month_end'] = (feature_df['day'] >= 25).astype(int)
        
        # å­£ç¯€æ€§ç‰¹å¾´é‡ï¼ˆã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯ï¼‰
        feature_df['month_sin'] = np.sin(2 * np.pi * feature_df['month'] / 12)
        feature_df['month_cos'] = np.cos(2 * np.pi * feature_df['month'] / 12)
        feature_df['dayofweek_sin'] = np.sin(2 * np.pi * feature_df['dayofweek'] / 7)
        feature_df['dayofweek_cos'] = np.cos(2 * np.pi * feature_df['dayofweek'] / 7)
        
        # å•†å“åˆ¥ç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã‚½ãƒ¼ãƒˆ
        feature_df = feature_df.sort_values(['product_id', 'date'])
        
        # ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆéå»ã®å€¤ï¼‰
        lag_periods = [1, 2, 3, 7, 14, 30]
        for lag in lag_periods:
            feature_df[f'quantity_lag_{lag}'] = feature_df.groupby('product_id')['quantity'].shift(lag)
        
        # ç§»å‹•å¹³å‡ç‰¹å¾´é‡
        window_sizes = [3, 7, 14, 30]
        for window in window_sizes:
            feature_df[f'quantity_ma_{window}'] = (
                feature_df.groupby('product_id')['quantity']
                .rolling(window=window, min_periods=1)
                .mean()
                .reset_index(level=0, drop=True)
            )
        
        # ç§»å‹•æ¨™æº–åå·®ï¼ˆå¤‰å‹•æ€§ï¼‰
        for window in [7, 14, 30]:
            feature_df[f'quantity_std_{window}'] = (
                feature_df.groupby('product_id')['quantity']
                .rolling(window=window, min_periods=1)
                .std()
                .reset_index(level=0, drop=True)
            )
        
        # æŒ‡æ•°ç§»å‹•å¹³å‡
        for alpha in [0.1, 0.3, 0.5]:
            feature_df[f'quantity_ema_{alpha}'] = (
                feature_df.groupby('product_id')['quantity']
                .ewm(alpha=alpha)
                .mean()
                .reset_index(level=0, drop=True)
            )
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡
        feature_df['quantity_trend'] = (
            feature_df.groupby('product_id')['quantity']
            .pct_change()
            .fillna(0)
        )
        
        # å•†å“IDã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        le = LabelEncoder()
        feature_df['product_id_encoded'] = le.fit_transform(feature_df['product_id'])
        
        # å¤–éƒ¨ç‰¹å¾´é‡ï¼ˆç¥æ—¥ã€ã‚¤ãƒ™ãƒ³ãƒˆãªã©ï¼‰
        feature_df = self.add_external_features(feature_df)
        
        # æ¬ æå€¤ã‚’åŸ‹ã‚ã‚‹
        feature_df = feature_df.fillna(method='ffill').fillna(0)
        
        print(f"ç‰¹å¾´é‡ä½œæˆå®Œäº†: {feature_df.shape[1]}æ¬¡å…ƒ")
        
        return feature_df
    
    def add_external_features(self, df):
        """å¤–éƒ¨ç‰¹å¾´é‡ã®è¿½åŠ """
        # æ—¥æœ¬ã®ç¥æ—¥ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        holidays = [
            '2023-01-01', '2023-01-09', '2023-02-11', '2023-02-23',
            '2023-03-21', '2023-04-29', '2023-05-03', '2023-05-04',
            '2023-05-05', '2023-07-17', '2023-08-11', '2023-09-18',
            '2023-09-23', '2023-10-09', '2023-11-03', '2023-11-23',
            '2023-12-29', '2023-12-30', '2023-12-31'
        ]
        
        holiday_dates = pd.to_datetime(holidays)
        df['is_holiday'] = df['date'].isin(holiday_dates).astype(int)
        
        # å¤§å‹é€£ä¼‘ãƒ•ãƒ©ã‚°
        golden_week = pd.date_range('2023-04-29', '2023-05-07')
        summer_vacation = pd.date_range('2023-08-11', '2023-08-20')
        year_end = pd.date_range('2023-12-29', '2023-12-31')
        
        df['is_golden_week'] = df['date'].isin(golden_week).astype(int)
        df['is_summer_vacation'] = df['date'].isin(summer_vacation).astype(int)
        df['is_year_end'] = df['date'].isin(year_end).astype(int)
        
        # ã‚»ãƒ¼ãƒ«æœŸé–“ï¼ˆä»®æƒ³çš„ï¼‰
        spring_sale = pd.date_range('2023-03-01', '2023-03-31')
        summer_sale = pd.date_range('2023-07-01', '2023-07-31')
        winter_sale = pd.date_range('2023-11-01', '2023-11-30')
        
        df['is_sale_period'] = (
            df['date'].isin(spring_sale) | 
            df['date'].isin(summer_sale) | 
            df['date'].isin(winter_sale)
        ).astype(int)
        
        # å¤©å€™æƒ…å ±ï¼ˆç°¡æ˜“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        np.random.seed(42)
        df['temperature'] = 20 + 10 * np.sin(2 * np.pi * df['dayofyear'] / 365) + np.random.normal(0, 3, len(df))
        df['is_rainy'] = (np.random.random(len(df)) < 0.3).astype(int)
        
        return df
    
    def prepare_ml_data(self, feature_df, target_col='quantity'):
        """æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        # ç‰¹å¾´é‡åˆ—ã‚’é¸æŠï¼ˆæ—¥ä»˜ã€å•†å“IDã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä»¥å¤–ï¼‰
        exclude_columns = ['date', 'product_id', target_col]
        feature_columns = [col for col in feature_df.columns if col not in exclude_columns]
        
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        X = feature_df[feature_columns]
        y = feature_df[target_col]
        
        # æ¬ æå€¤ãŒã‚ã‚‹è¡Œã‚’é™¤å¤–
        mask = ~(X.isna().any(axis=1) | y.isna())
        X = X[mask]
        y = y[mask]
        
        self.feature_columns = feature_columns
        
        print(f"MLç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {X.shape[0]}ã‚µãƒ³ãƒ—ãƒ«, {X.shape[1]}ç‰¹å¾´é‡")
        
        return X, y
    
    def train_random_forest(self, X, y, test_size=0.2):
        """Random Forest ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        print("Random Forest ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=None
        )
        
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        
        rf = RandomForestRegressor(random_state=42, n_jobs=-1)
        
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆæ™‚é–“çŸ­ç¸®ã®ãŸã‚å°ã•ãªã‚µãƒ³ãƒ—ãƒ«ã§ï¼‰
        if len(X_train) > 10000:
            sample_indices = np.random.choice(len(X_train), 5000, replace=False)
            X_sample = X_train.iloc[sample_indices]
            y_sample = y_train.iloc[sample_indices]
        else:
            X_sample = X_train
            y_sample = y_train
        
        grid_search = GridSearchCV(
            rf, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1
        )
        
        grid_search.fit(X_sample, y_sample)
        best_rf = grid_search.best_estimator_
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã§å†è¨“ç·´
        best_rf.fit(X_train, y_train)
        
        # äºˆæ¸¬ã¨è©•ä¾¡
        y_pred = best_rf.predict(X_test)
        
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        
        self.models['random_forest'] = {
            'model': best_rf,
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'feature_importance': dict(zip(self.feature_columns, best_rf.feature_importances_))
        }
        
        print(f"Random Forest è¨“ç·´å®Œäº† - MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.3f}")
        
        return best_rf, X_test, y_test, y_pred
    
    def train_xgboost(self, X, y, test_size=0.2):
        """XGBoost ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        print("XGBoost ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
        
        # XGBoost ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        params = {
            'objective': 'reg:squarederror',
            'eval_metric': 'mae',
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 200,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42
        }
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        xgb_model = xgb.XGBRegressor(**params)
        
        # Early stopping
        xgb_model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            early_stopping_rounds=20,
            verbose=False
        )
        
        # äºˆæ¸¬ã¨è©•ä¾¡
        y_pred = xgb_model.predict(X_test)
        
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        
        # ç‰¹å¾´é‡é‡è¦åº¦
        feature_importance = dict(zip(self.feature_columns, xgb_model.feature_importances_))
        
        self.models['xgboost'] = {
            'model': xgb_model,
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'feature_importance': feature_importance
        }
        
        print(f"XGBoost è¨“ç·´å®Œäº† - MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.3f}")
        
        return xgb_model, X_test, y_test, y_pred
    
    def prepare_lstm_data(self, df, lookback_days=30, product_id=None):
        """LSTMç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        if product_id:
            df = df[df['product_id'] == product_id].copy()
        
        # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã«ãƒªã‚µãƒ³ãƒ—ãƒ«
        df = df.set_index('date')['quantity'].resample('D').sum().fillna(0)
        
        # æ­£è¦åŒ–
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(df.values.reshape(-1, 1)).flatten()
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        X, y = [], []
        for i in range(lookback_days, len(scaled_data)):
            X.append(scaled_data[i-lookback_days:i])
            y.append(scaled_data[i])
        
        X = np.array(X)
        y = np.array(y)
        
        # LSTMç”¨ã«æ¬¡å…ƒå¤‰æ›´ (samples, time steps, features)
        X = X.reshape(X.shape[0], X.shape[1], 1)
        
        self.scalers['lstm'] = scaler
        
        return X, y, scaler
    
    def train_lstm(self, df, lookback_days=30, product_id='PROD_0001'):
        """LSTM ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        print(f"LSTM ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­ (å•†å“ID: {product_id})...")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X, y, scaler = self.prepare_lstm_data(df, lookback_days, product_id)
        
        if len(X) < 100:
            print("LSTMè¨“ç·´ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return None, None, None, None
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        # LSTM ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        model = Sequential([
            LSTM(50, return_sequences=True, input_shape=(lookback_days, 1)),
            Dropout(0.2),
            LSTM(50, return_sequences=False),
            Dropout(0.2),
            Dense(25),
            Dense(1)
        ])
        
        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
        
        # Early stopping
        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        history = model.fit(
            X_train, y_train,
            batch_size=32,
            epochs=100,
            validation_data=(X_test, y_test),
            callbacks=[early_stop],
            verbose=0
        )
        
        # äºˆæ¸¬ã¨è©•ä¾¡
        y_pred_scaled = model.predict(X_test, verbose=0)
        
        # æ­£è¦åŒ–ã‚’å…ƒã«æˆ»ã™
        y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
        y_pred_original = scaler.inverse_transform(y_pred_scaled).flatten()
        
        mae = mean_absolute_error(y_test_original, y_pred_original)
        rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))
        r2 = r2_score(y_test_original, y_pred_original)
        
        self.models['lstm'] = {
            'model': model,
            'scaler': scaler,
            'lookback_days': lookback_days,
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'history': history
        }
        
        print(f"LSTM è¨“ç·´å®Œäº† - MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.3f}")
        
        return model, X_test, y_test_original, y_pred_original
    
    def visualize_feature_importance(self):
        """ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        # Random Forest ã®ç‰¹å¾´é‡é‡è¦åº¦
        if 'random_forest' in self.models:
            rf_importance = self.models['random_forest']['feature_importance']
            top_features = dict(sorted(rf_importance.items(), key=lambda x: x[1], reverse=True)[:15])
            
            ax1 = axes[0]
            bars = ax1.barh(list(top_features.keys()), list(top_features.values()), color='skyblue')
            ax1.set_title('Random Forest ç‰¹å¾´é‡é‡è¦åº¦ (Top 15)')
            ax1.set_xlabel('é‡è¦åº¦')
            
            # æ•°å€¤ãƒ©ãƒ™ãƒ«è¿½åŠ 
            for bar, value in zip(bars, top_features.values()):
                ax1.text(value + 0.001, bar.get_y() + bar.get_height()/2, 
                        f'{value:.3f}', va='center', ha='left')
        
        # XGBoost ã®ç‰¹å¾´é‡é‡è¦åº¦
        if 'xgboost' in self.models:
            xgb_importance = self.models['xgboost']['feature_importance']
            top_features = dict(sorted(xgb_importance.items(), key=lambda x: x[1], reverse=True)[:15])
            
            ax2 = axes[1]
            bars = ax2.barh(list(top_features.keys()), list(top_features.values()), color='lightcoral')
            ax2.set_title('XGBoost ç‰¹å¾´é‡é‡è¦åº¦ (Top 15)')
            ax2.set_xlabel('é‡è¦åº¦')
            
            # æ•°å€¤ãƒ©ãƒ™ãƒ«è¿½åŠ 
            for bar, value in zip(bars, top_features.values()):
                ax2.text(value + 0.001, bar.get_y() + bar.get_height()/2, 
                        f'{value:.3f}', va='center', ha='left')
        
        plt.tight_layout()
        
        # ä¿å­˜
        output_dir = Path("results/ml_forecasting")
        output_dir.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_dir / "feature_importance.png", dpi=300, bbox_inches='tight')
        
        return fig
    
    def visualize_model_comparison(self):
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒã®å¯è¦–åŒ–"""
        if not self.models:
            print("æ¯”è¼ƒã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # æ€§èƒ½æŒ‡æ¨™ã‚’ã¾ã¨ã‚ã‚‹
        model_names = list(self.models.keys())
        mae_scores = [self.models[name]['mae'] for name in model_names]
        rmse_scores = [self.models[name]['rmse'] for name in model_names]
        r2_scores = [self.models[name]['r2'] for name in model_names]
        
        # å¯è¦–åŒ–
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # MAEæ¯”è¼ƒ
        ax1 = axes[0]
        bars1 = ax1.bar(model_names, mae_scores, color=['skyblue', 'lightcoral', 'lightgreen'][:len(model_names)])
        ax1.set_title('å¹³å‡çµ¶å¯¾èª¤å·® (MAE) æ¯”è¼ƒ')
        ax1.set_ylabel('MAE')
        
        for bar, value in zip(bars1, mae_scores):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{value:.2f}', ha='center', va='bottom')
        
        # RMSEæ¯”è¼ƒ
        ax2 = axes[1]
        bars2 = ax2.bar(model_names, rmse_scores, color=['skyblue', 'lightcoral', 'lightgreen'][:len(model_names)])
        ax2.set_title('å¹³æ–¹æ ¹å¹³å‡äºŒä¹—èª¤å·® (RMSE) æ¯”è¼ƒ')
        ax2.set_ylabel('RMSE')
        
        for bar, value in zip(bars2, rmse_scores):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{value:.2f}', ha='center', va='bottom')
        
        # RÂ²æ¯”è¼ƒ
        ax3 = axes[2]
        bars3 = ax3.bar(model_names, r2_scores, color=['skyblue', 'lightcoral', 'lightgreen'][:len(model_names)])
        ax3.set_title('æ±ºå®šä¿‚æ•° (RÂ²) æ¯”è¼ƒ')
        ax3.set_ylabel('RÂ²')
        ax3.set_ylim(0, 1)
        
        for bar, value in zip(bars3, r2_scores):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                    f'{value:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        # ä¿å­˜
        output_dir = Path("results/ml_forecasting")
        output_dir.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_dir / "model_comparison.png", dpi=300, bbox_inches='tight')
        
        return fig
    
    def save_models(self):
        """è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        output_dir = Path("models")
        output_dir.mkdir(exist_ok=True)
        
        for model_name, model_info in self.models.items():
            if model_name == 'lstm':
                # Kerasãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
                model_info['model'].save(output_dir / f"{model_name}_model.keras")
                # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä¿å­˜
                joblib.dump(model_info['scaler'], output_dir / f"{model_name}_scaler.pkl")
            else:
                # scikit-learnã€XGBoostãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
                joblib.dump(model_info['model'], output_dir / f"{model_name}_model.pkl")
        
        # ç‰¹å¾´é‡åˆ—ã®ä¿å­˜
        joblib.dump(self.feature_columns, output_dir / "feature_columns.pkl")
        
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {output_dir}")
    
    def generate_ml_forecast_report(self):
        """æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        print("\n" + "="*60)
        print("æ©Ÿæ¢°å­¦ç¿’éœ€è¦äºˆæ¸¬ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        
        if not self.models:
            print("è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å®š
        best_model_name = min(self.models.keys(), key=lambda x: self.models[x]['mae'])
        best_model = self.models[best_model_name]
        
        print(f"\nğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model_name}")
        print(f"   MAE: {best_model['mae']:.2f}")
        print(f"   RMSE: {best_model['rmse']:.2f}")
        print(f"   RÂ²: {best_model['r2']:.3f}")
        
        print(f"\nğŸ“Š å…¨ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ:")
        for model_name, model_info in self.models.items():
            print(f"  {model_name}:")
            print(f"    MAE: {model_info['mae']:.2f}")
            print(f"    RMSE: {model_info['rmse']:.2f}")
            print(f"    RÂ²: {model_info['r2']:.3f}")
        
        # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆRandom Forest/XGBoostã®ã¿ï¼‰
        if 'random_forest' in self.models:
            rf_importance = self.models['random_forest']['feature_importance']
            top_features = dict(sorted(rf_importance.items(), key=lambda x: x[1], reverse=True)[:5])
            
            print(f"\nğŸ” Random Forest é‡è¦ç‰¹å¾´é‡ Top 5:")
            for feature, importance in top_features.items():
                print(f"  {feature}: {importance:.4f}")
        
        return best_model_name, best_model

def create_comprehensive_demand_data():
    """åŒ…æ‹¬çš„ãªã‚µãƒ³ãƒ—ãƒ«éœ€è¦ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
    np.random.seed(42)
    
    # è¤‡æ•°å•†å“ã€1å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿
    products = [f"PROD_{i:04d}" for i in range(1, 21)]  # 20å•†å“
    dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
    
    demand_data = []
    
    for product_id in products:
        # å•†å“ã”ã¨ã®ç‰¹æ€§
        base_demand = np.random.uniform(20, 100)
        seasonality_strength = np.random.uniform(0.1, 0.3)
        trend_strength = np.random.uniform(-0.1, 0.2)
        noise_level = np.random.uniform(0.1, 0.4)
        
        for date in dates:
            day_of_year = date.timetuple().tm_yday
            
            # å­£ç¯€æ€§
            seasonal_effect = seasonality_strength * np.sin(2 * np.pi * day_of_year / 365)
            
            # é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³
            weekly_effect = 0.2 * np.sin(2 * np.pi * date.weekday() / 7)
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰
            trend_effect = trend_strength * day_of_year / 365
            
            # ç¥æ—¥åŠ¹æœ
            holiday_effect = 0
            if date.weekday() >= 5:  # é€±æœ«
                holiday_effect = 0.3
            
            # åŸºæœ¬éœ€è¦è¨ˆç®—
            expected_demand = base_demand * (
                1 + seasonal_effect + weekly_effect + trend_effect + holiday_effect
            )
            
            # ãƒã‚¤ã‚ºè¿½åŠ 
            actual_demand = max(0, np.random.normal(expected_demand, expected_demand * noise_level))
            
            demand_data.append({
                'date': date,
                'product_id': product_id,
                'quantity': int(actual_demand)
            })
    
    df = pd.DataFrame(demand_data)
    
    # ä¿å­˜
    output_dir = Path("data/sample_data")
    output_dir.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_dir / "ml_demand_data.csv", index=False, encoding='utf-8-sig')
    print(f"åŒ…æ‹¬çš„ã‚µãƒ³ãƒ—ãƒ«éœ€è¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ: {output_dir / 'ml_demand_data.csv'}")
    
    return df

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("æ©Ÿæ¢°å­¦ç¿’éœ€è¦äºˆæ¸¬ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    if not Path("data/sample_data/ml_demand_data.csv").exists():
        print("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
        create_comprehensive_demand_data()
    
    # äºˆæ¸¬å™¨ã®åˆæœŸåŒ–
    forecaster = MLDemandForecaster()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = forecaster.load_and_prepare_data("data/sample_data/ml_demand_data.csv")
    if df is None:
        return
    
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    feature_df = forecaster.create_features(df)
    
    # MLç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X, y = forecaster.prepare_ml_data(feature_df)
    
    # Random Forest è¨“ç·´
    print("\n" + "="*50)
    rf_model, X_test_rf, y_test_rf, y_pred_rf = forecaster.train_random_forest(X, y)
    
    # XGBoost è¨“ç·´
    print("\n" + "="*50)
    xgb_model, X_test_xgb, y_test_xgb, y_pred_xgb = forecaster.train_xgboost(X, y)
    
    # LSTM è¨“ç·´
    print("\n" + "="*50)
    lstm_model, X_test_lstm, y_test_lstm, y_pred_lstm = forecaster.train_lstm(df)
    
    # å¯è¦–åŒ–
    print("\nå¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
    forecaster.visualize_feature_importance()
    forecaster.visualize_model_comparison()
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    forecaster.save_models()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    best_model_name, best_model = forecaster.generate_ml_forecast_report()
    
    print(f"\nâœ… æ©Ÿæ¢°å­¦ç¿’éœ€è¦äºˆæ¸¬ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"ğŸ† æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {best_model_name}")
    print("ğŸ“Š results/ml_forecasting/ ãƒ•ã‚©ãƒ«ãƒ€ã«çµæœãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚")
    print("ğŸ’¾ models/ ãƒ•ã‚©ãƒ«ãƒ€ã«è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚")

if __name__ == "__main__":
    main()
